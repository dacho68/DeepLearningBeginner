{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning\n",
    "* It forces a standar structure on your code - easier to reason about your code (and someone else's)\n",
    "* It gets rid of a lot of boilerplate\n",
    "* It abstracts away a lot of extra stuff besides the core code - logging and parallelization on GPU or even TPU. Paralleization can be done by just adding some flags to your Trainer rather than refactoring your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "Build a model\n",
    "Define loss_func and optimizer\n",
    "Define trainer\n",
    "Define test\n",
    "Run trainer and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MNIST(root='data', train = True, download=True, transform=ToTensor())\n",
    "valid_ds = MNIST(root='data', train = False, download=True, transform=ToTensor())\n",
    "\n",
    "bs = 64\n",
    "train_dl = DataLoader(train_ds,batch_size=bs,shuffle=True,num_workers=15)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=bs,shuffle=False,num_workers=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#build a model\n",
    "from typing import Any\n",
    "import torchmetrics\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self, lr = 0.5):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.lin = nn.Linear(784,10)\n",
    "        # self.lin2 = nn.Linear(64,32)\n",
    "        # self.lin3 = nn.Linear(32,10)\n",
    "        #metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.valid_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward (self, xb):\n",
    "        xb = xb.flatten(1,-1) # (bs,1,28,28) -> bs, 784\n",
    "        return self.lin(xb)\n",
    "    \n",
    "\n",
    "    def shared_step(self, batch, train):\n",
    "        xb, yb = batch\n",
    "        pred = self.forward(xb)\n",
    "        loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "        #logginh\n",
    "        if (train):\n",
    "            self.train_accuracy(pred.softmax(dim=-1),yb)\n",
    "            self.log('train_accuracy',self.train_accuracy,on_step=True,on_epoch=False, prog_bar=True)\n",
    "        else:\n",
    "            self.valid_accuracy(pred.softmax(dim=-1),yb)\n",
    "            self.log('valid_accuracy',self.valid_accuracy,on_step=True,on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch,batch_idx):\n",
    "        return  self.shared_step(batch,train=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return  self.shared_step(batch,train=False)\n",
    "    \n",
    "    def configure_optimizers(self) :\n",
    "        return optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | lin            | Linear             | 7.9 K \n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | valid_accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dac\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dac\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:23<00:00, 39.34it/s, v_num=2, train_accuracy=0.938, valid_accuracy=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:23<00:00, 39.33it/s, v_num=2, train_accuracy=0.938, valid_accuracy=1.000]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# >tensorboard --logdir tb_logs\n",
    "\n",
    "tb_logger = TensorBoardLogger('tb_logs', name=\"my_model\")\n",
    "# init model\n",
    "mnist_model = MNISTModel()\n",
    "\n",
    "#init trainer\n",
    "trainer = pl.Trainer(max_epochs=2,logger=tb_logger)\n",
    "\n",
    "#training the model\n",
    "trainer.fit(mnist_model,train_dl, valid_dl)\n",
    "\n",
    "#optinally - run test\n",
    "#trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
